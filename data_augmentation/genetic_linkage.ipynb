{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import norm\n",
    "\n",
    "## Data imputation algorithm\n",
    "\n",
    "# Idea is that over iterations, you are updating your current approximation to the posterior distribution\n",
    "# And over imputations, you are generating a sample from the current approximation to the posterior distribution\n",
    "# So, each imputation is generated from the current approximation to the posterior distribution\n",
    "\n",
    "## Genetic linkage example\n",
    "\n",
    "# Observed data Y = (125, 18, 20, 34)\n",
    "# For mini genetic linkage model Y = (14, 0, 1, 5)\n",
    "# And the last observed data option is Y = (13, 1, 2, 3)\n",
    "# Assuming uniform prior on theta\n",
    "\n",
    "y1, y2, y3, y4 = [13, 1, 2, 3]\n",
    "\n",
    "numIterations = 100\n",
    "numImputations = 5000\n",
    "posteriorApproximations = np.zeros([numIterations, numImputations])\n",
    "randomIndices = random.choices(range(numImputations), k = numIterations)\n",
    "currentTheta = np.random.uniform(0, 1, numImputations)\n",
    "\n",
    "for iiIteration in range(numIterations):\n",
    "\n",
    "    # Display every n'th iteration\n",
    "\n",
    "    if iiIteration % 100 == 0:\n",
    "        \n",
    "        print('Iteration: ', iiIteration)\n",
    "\n",
    "    # Generate m imputations / latent variables from the current posterior distribution (represented by a sample of thetas not just one theta)\n",
    "\n",
    "    currentImputations = np.zeros([numImputations])\n",
    "\n",
    "    for jjImputation in range(numImputations):\n",
    "\n",
    "        thetaStar = currentTheta[jjImputation]\n",
    "        currentImputations[jjImputation] = np.random.binomial(y1, thetaStar / (thetaStar + 2), 1)\n",
    "\n",
    "    # Now sample theta star from Beta(v1, v2) where v1 = x2 + y4 + 1 and v2 = y2 + y3\n",
    "\n",
    "    for jjImputation in range(numImputations):\n",
    "\n",
    "        currentLatentVariable = currentImputations[random.randint(0, numImputations - 1)] # drawn from the conditional predictive distribution given the current approximation to the posterior\n",
    "        currentTheta[jjImputation] = np.random.beta(currentLatentVariable + y4 + 1, y2 + y3 + 1, 1) # sampling the current approximation to the posterior\n",
    "        # (which of course depends on the latent variables that in turn depend on the previous approximation to the posterior)\n",
    "        posteriorApproximations[iiIteration, jjImputation] = currentTheta[jjImputation]\n",
    "\n",
    "    if iiIteration == (numIterations - 1):\n",
    "\n",
    "        print('Arrived at final approximation to the posterior distribution')\n",
    "\n",
    "\n",
    "# Generate the pdfs to plot as well\n",
    "\n",
    "posteriorSample = currentTheta\n",
    "kde1 = gaussian_kde(posteriorSample)\n",
    "xValues1 = np.linspace(min(posteriorSample), max(posteriorSample), numIterations)\n",
    "densityValues1 = kde1(xValues1)\n",
    "\n",
    "## IDEA FOR PLOTTING --- PLOT EACH APPROXIMATION TO THE POSTERIOR FOR DIFFERENT ITERATIONS (I.E. ITERATION = 1, 50, 100, 100000) ETC.\n",
    "\n",
    "plt.hist(posteriorSample, density = True, color = 'skyblue', edgecolor = 'black')\n",
    "plt.plot(xValues1, densityValues1, color = 'black', label = 'Kernel density approximation')\n",
    "print('Adding labels')\n",
    "plt.rc('text', usetex = True)\n",
    "plt.xlabel('$\\\\theta$')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Augmented Posterior Distribution for $\\\\theta$')\n",
    "\n",
    "sampleMean = np.mean(posteriorSample)\n",
    "sampleStd = np.std(posteriorSample)\n",
    "theta = np.linspace(0, 1, 10000)\n",
    "normalApprox = norm.pdf(theta, loc = sampleMean, scale = sampleStd)\n",
    "plt.plot(theta, normalApprox, color = 'red', label = 'Normal approximation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## Plot the convergence of the process (i.e. plot mean of jointPosteriorApprox over iteration)\n",
    "## Or could plot percentiles or something like that over iterations on a graph and see where it stabilizes \n",
    "\n",
    "quantiles = np.array([.025, .50, .975])\n",
    "convergenceQuantiles = np.quantile(posteriorApproximations, quantiles, axis = 1) \n",
    "\n",
    "## COMPARE TO THE GENETIC LINKAGE MINISET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array(['blue', 'red', 'green'])\n",
    "labels = np.array(['.025 quantile', '.50 quantile', '.975 quantile'])\n",
    "\n",
    "for iiQuantile in range(len(quantiles)):\n",
    "\n",
    "    plt.plot(convergenceQuantiles[iiQuantile, :], color = colors[iiQuantile], label = labels[iiQuantile])\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.rc('text', usetex = True)\n",
    "plt.ylabel('$\\\\theta$')\n",
    "plt.title('Convergence of Posterior Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
